# Optimization

## 1. Theory

Automatic differentiation is fundamental to DMFF and aids in neural network optimization. During training, it computes the derivatives from output to input using backpropagation, optimizing parameters through gradient descent. With its efficiency in optimizing high-dimensional parameters, this technique isn't limited to neural networks but suits any framework following the "input parameters → model computation → output" sequence, such as molecular dynamics (MD) simulations. Hence, using automatic differentiation and referencing experimental or ab initio data, we can optimize force field parameters by computing the output's derivative with respect to input parameters.

## 2. Function module 

Imports: Importing necessary modules and functions from `jax` and `optax`.

Function `periodic_move`:
- Creates a function to perform a periodic update on parameters. If the update causes the parameters to exceed a given range, they are wrapped around in a periodic manner (like an angle that wraps around after 360 degrees).

Function `genOptimizer`:
- It's a function to generate an optimizer based on user preferences.
- Depending on the arguments, it can produce various optimization schemes, such as SGD, Nesterov, Adam, and others.
- Supports learning rate schedules like exponential decay and warmup exponential decay.
- The optimizer can be further augmented with features like gradient clipping, periodic parameter wrapping, and keeping parameters non-negative.

Function `label_iter`, `mark_iter`, and `label2trans_iter`:
- These are utility functions used for tree-like structured data (common with JAX's pytree concept).
- `label_iter` recursively labels the tree nodes.
- `mark_iter marks` each node in the tree with a False.
- `label2trans_iter` checks and updates the mask tree based on whether the label exists in the transformations. If not, it sets that transformation to zero.

Class `MultiTransform`:
- Manages multiple transformations simultaneously on tree-structured data.
- Maintains a record of transformations, labels, and masks.
- `finalize` method ensures that every label has a corresponding transformation, setting any missing transformations to zero.

## 3. How to use it

To set up an optimization, you should follow these steps:
- Initialize MultiTransform with Parameter Tree:

```python
multiTrans = MultiTransform(your_parameter_tree)
```
- Define Optimizers for Desired Labels

- For each part of the parameter tree you want to optimize differently, set an optimizer. For example:
  
```python
multiTrans["Label/Path1"] = genOptimizer(learning_rate=lr1, clip=clip1)
multiTrans["Label/Path2"] = genOptimizer(learning_rate=lr2, clip=clip2)
```

- Finalize MultiTransform

- Before using it, always finalize the MultiTransform:
  
```python
multiTrans.finalize()
```

- Create a Combined Gradient Transformation:
  
```python
traj = md.load("init.dcd", top="box.pdb")[50:]
```

- Create a sample using the loaded trajectory and the previously defined state name:
  
```python
grad_transform = optax.multi_transform(multiTrans.transforms, multiTrans.labels)
```

- Mask Integer Parameters (If Needed)

- If you have parameters in your tree that shouldn't be updated, create a mask and then mask your transformation:
  
```python
mask = jax.tree_util.tree_map(lambda x: x.dtype != jnp.int32 and x.dtype != int, your_parameter_tree)
grad_transform = optax.masked(grad_transform, mask)
```

- Initialize Optimization State:
  
```python
opt_state = grad_transform.init(your_parameter_tree)
```

By following the above steps, you'll have a `grad_transform` that can handle complex parameter trees and an optimization state `opt_state` ready for your optimization routine.
